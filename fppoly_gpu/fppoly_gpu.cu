/*
 *
 *  This source file is part of ELINA (ETH LIbrary for Numerical Analysis).
 *  ELINA is Copyright Â© 2018 Department of Computer Science, ETH Zurich
 *  This software is distributed under GNU Lesser General Public License
 * Version 3.0. For more information, see the ELINA project website at:
 *  http://elina.ethz.ch
 *
 *  THE SOFTWARE IS PROVIDED "AS-IS" WITHOUT ANY WARRANTY OF ANY KIND, EITHER
 *  EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO ANY WARRANTY
 *  THAT THE SOFTWARE WILL CONFORM TO SPECIFICATIONS OR BE ERROR-FREE AND ANY
 *  IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE,
 *  TITLE, OR NON-INFRINGEMENT.  IN NO EVENT SHALL ETH ZURICH BE LIABLE FOR ANY
 *  DAMAGES, INCLUDING BUT NOT LIMITED TO DIRECT, INDIRECT,
 *  SPECIAL OR CONSEQUENTIAL DAMAGES, ARISING OUT OF, RESULTING FROM, OR IN
 *  ANY WAY CONNECTED WITH THIS SOFTWARE (WHETHER OR NOT BASED UPON WARRANTY,
 *  CONTRACT, TORT OR OTHERWISE).
 *
 */

#include "fppoly_gpu.h"

#include <chrono>
#include <cuda.h>
#include <iostream>

const size_t num_threads = 128;

bool results[90];
bool results_calculated;
size_t output_counter;

#ifdef single
__constant__ const float_type min_denormal = 1.40129846e-45;
__constant__ const float_type ulp = 1.1920929e-07;
#else
__constant__ const float_type min_denormal = 4.940656458412465441766e-324;
__constant__ const float_type ulp = 2.220446049250313080848e-16;
#endif

#define gpuErrchk(ans)                                                         \
  { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line,
                      bool abort = true) {
  if (code != cudaSuccess) {
    fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file,
            line);
    if (abort)
      exit(code);
  }
}

__device__ void
elina_double_interval_mul(float_type *const a_inf, float_type *const a_sup,
                          const float_type b_inf, const float_type b_sup,
                          const float_type c_inf, const float_type c_sup) {
  if (c_inf <= 0) {
    /* interval c is positive */
    if (b_inf <= 0) {
      /*interval b is positive*/
      if ((b_inf == 0) || (c_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_inf * -c_inf;
      }

      if ((b_sup == 0) || (c_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_sup * c_sup;
      }
    } else if (b_sup <= 0) {
      /* interval b is negative */
      if ((c_sup == 0) || (b_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = c_sup * b_inf;
      }

      if ((c_inf == 0) || (b_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = -c_inf * b_sup;
      }
    } else {
      /* there is 0 in between for b */
      if ((c_sup == 0) || (b_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_inf * c_sup;
      }

      if ((c_sup == 0) || (b_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_sup * c_sup;
      }
    }
  } else if (c_sup <= 0) {
    /* interval c is negative */
    if (b_inf <= 0) {
      /*interval b is positive*/
      if ((b_sup == 0) || (c_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_sup * c_inf;
      }

      if ((b_inf == 0) || (c_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = -b_inf * c_sup;
      }
    } else if (b_sup <= 0) {
      /* interval b is negative */
      if ((b_sup == 0) || (c_sup == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_sup * -c_sup;
      }

      if ((b_inf == 0) || (c_inf == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_inf * c_inf;
      }
    } else {
      /* there is 0 in between for b */
      if ((c_inf == 0) || (b_sup == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_sup * c_inf;
      }

      if ((c_inf == 0) || (b_inf == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_inf * c_inf;
      }
    }
  } else if (b_inf <= 0) {
    /* interval b is positive */
    if (c_inf <= 0) {
      /*interval c is positive */
      if ((b_inf == 0) || (c_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = -b_inf * c_inf;
      }

      if ((b_sup == 0) || (c_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_sup * c_sup;
      }
    } else if (c_sup <= 0) {
      /* interval c is negative */
      if ((b_sup == 0) || (c_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_sup * c_inf;
      }

      if ((b_inf == 0) || (c_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = -b_inf * c_sup;
      }
    } else {
      /* there is 0 in between for c */
      if ((b_sup == 0) || (c_inf == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_sup * c_inf;
      }

      if ((b_sup == 0) || (c_sup == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_sup * c_sup;
      }
    }
  } else if (b_sup <= 0) {
    /* interval b is negative */
    if (c_inf <= 0) {
      /* interval c is positive */
      if ((b_inf == 0) || (c_sup == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_inf * c_sup;
      }

      if ((b_sup == 0) || (c_inf == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_sup * -c_inf;
      }
    } else if (c_sup <= 0) {
      /* interval c is negative */
      if ((b_sup == 0) || (c_sup == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = -b_sup * c_sup;
      }

      if ((b_inf == 0) || (c_inf == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_inf * c_inf;
      }
    } else {
      /* there is 0 in between for c */
      if ((b_inf == 0) || (c_sup == 0)) {
        *a_inf = 0.0;
      } else {
        *a_inf = b_inf * c_sup;
      }

      if ((b_inf == 0) || (c_inf == 0)) {
        *a_sup = 0.0;
      } else {
        *a_sup = b_inf * c_inf;
      }
    }
  } else {
    /* there is 0 in between for both b and c */
    float_type tmp_inf1 = b_sup * c_inf;
    float_type tmp_sup1 = b_inf * c_inf;
    float_type tmp_inf2 = b_inf * c_sup;
    float_type tmp_sup2 = b_sup * c_sup;
    *a_inf = fmax(tmp_inf1, tmp_inf2);
    *a_sup = fmax(tmp_sup1, tmp_sup2);
  }
}

__device__ void
elina_double_interval_div(float_type *const a_inf, float_type *const a_sup,
                          const float_type b_inf, const float_type b_sup,
                          const float_type c_inf, const float_type c_sup) {
  if (c_inf < 0) {
    /* c is positive */
    if (b_inf <= 0) {
      /* b is positive */
      *a_inf = b_inf / c_sup;
      *a_sup = b_sup / -c_inf;
    } else if (b_sup <= 0) {
      /* b is negative */
      *a_inf = -b_inf / c_inf;
      *a_sup = b_sup / c_sup;
    } else {
      /* 0 is in the middle of b: one divides b by c->inf */
      *a_inf = b_inf / -c_inf;
      *a_sup = b_sup / -c_inf;
    }
  } else if (c_sup < 0) {
    /* c is negative */
    if (b_inf <= 0) {
      /* b is positive */
      *a_sup = b_inf / c_inf;
      *a_inf = -b_sup / c_sup;
    } else if (b_sup <= 0) {
      /* b is negative */
      *a_inf = b_sup / c_inf;
      *a_sup = -b_inf / c_sup;
    } else {
      /* 0 is in the middle of b: one cross-divide b by c->sup */
      *a_inf = b_sup / c_sup;
      *a_sup = b_inf / c_sup;
    }
  } else if ((b_inf == 0) && (b_sup == 0)) {
    /* b is [0,0] */
    *a_inf = b_inf;
    *a_sup = b_sup;
  } else {
    *a_inf = INFINITY;
    *a_sup = INFINITY;
  }
}

fppoly_t *fppoly_of_abstract0(elina_abstract0_t *a) {
  return (fppoly_t *)a->value;
}

elina_abstract0_t *abstract0_of_fppoly(elina_manager_t *man, fppoly_t *fp) {
  elina_abstract0_t *r = (elina_abstract0_t *)malloc(sizeof(elina_abstract0_t));
  assert(r);
  r->value = fp;
  r->man = elina_manager_copy(man);

  return r;
}

static inline void fppoly_internal_free(fppoly_internal_t *pr) {
  if (pr) {
    pr->funid = ELINA_FUNID_UNKNOWN;
    free(pr);
    pr = nullptr;
  }
}

static inline fppoly_internal_t *fppoly_internal_alloc() {
  fppoly_internal_t *pr =
      (fppoly_internal_t *)malloc(sizeof(fppoly_internal_t));
  pr->funid = ELINA_FUNID_UNKNOWN;
  pr->man = nullptr;
  pr->funopt = nullptr;
  pr->min_denormal = ldexpl(1.0, -1074);
  pr->ulp = ldexpl(1.0, -52);

  return pr;
}

/* back pointer to our internal structure from the manager */
fppoly_internal_t *fppoly_init_from_manager(elina_manager_t *man,
                                            elina_funid_t funid) {
  fppoly_internal_t *pr = (fppoly_internal_t *)man->internal;
  pr->funid = funid;

  if (!(pr->man)) {
    pr->man = man;
  }

  return pr;
}

elina_manager_t *fppoly_manager_alloc() {
  std::cout << "This is the GPU version of fppoly!" << std::endl;
  results_calculated = false;
  output_counter = 1;

  void **funptr;
  fppoly_internal_t *pr = fppoly_internal_alloc();

  elina_manager_t *man = elina_manager_alloc(
      "fppoly",                              /* Library name */
      "1.0",                                 /* version */
      pr,                                    /* internal structure */
      (void (*)(void *))fppoly_internal_free /* free function for internal */
  );

  funptr = man->funptr;
  funptr[ELINA_FUNID_FREE] = (void *)&fppoly_free;
  /* 3.Printing */
  funptr[ELINA_FUNID_FPRINT] = (void *)&fppoly_fprint;

  return man;
}

/*
__device__
void expr_print(const expr_t* const expr)
{
    if((expr->inf_coeff == nullptr) || (expr->sup_coeff == nullptr))
    {
        printf("+ [%g, %g]\n", -expr->inf_cst, expr->sup_cst);

        return;
    }

    for(size_t i = 0; i < size; i++)
    {
        if(i == 0)
        {
            printf("[%g, %g]x0 ", -expr->inf_coeff[0], expr->sup_coeff[0]);
        }
        else
        {
            printf("+ [%g, %g]x%zu ", -expr->inf_coeff[i], expr->sup_coeff[i],
i);
        }
    }

    printf("+ [%g, %g]\n", -expr->inf_cst, expr->sup_cst);
}
*/

layer_t *create_layer(const size_t num_out_neurons, const size_t num_in_neurons,
                      const layertype_t type,
                      const activation_type_t activation) {
  layer_t *layer = (layer_t *)malloc(sizeof(layer_t));

  layer->num_out_neurons = num_out_neurons;
  layer->num_in_neurons = num_in_neurons;

  layer->type = type;
  layer->activation = activation;

  cudaMalloc((void **)&layer->lb_array, num_out_neurons * sizeof(float_type));
  cudaMalloc((void **)&layer->ub_array, num_out_neurons * sizeof(float_type));

  cudaMalloc((void **)&layer->inf_coeff,
             num_out_neurons * num_in_neurons * sizeof(float_type));
  cudaMalloc((void **)&layer->sup_coeff,
             num_out_neurons * num_in_neurons * sizeof(float_type));

  cudaMalloc((void **)&layer->inf_cst, num_out_neurons * sizeof(float_type));
  cudaMalloc((void **)&layer->sup_cst, num_out_neurons * sizeof(float_type));

  return layer;
}

void fppoly_from_network_input_box(fppoly_t *const res, const size_t intdim,
                                   const size_t realdim,
                                   const double *inf_array,
                                   const double *sup_array) {
  res->layers = nullptr;
  res->numlayers = 0;

  size_t num_pixels = intdim + realdim;

  float_type *tmp_input_inf =
      (float_type *)malloc(num_pixels * sizeof(float_type));
  float_type *tmp_input_sup =
      (float_type *)malloc(num_pixels * sizeof(float_type));

  for (size_t i = 0; i < num_pixels; i++) {
    tmp_input_inf[i] = -inf_array[i];
    tmp_input_sup[i] = sup_array[i];
  }

  cudaMalloc((void **)&(res->input_inf), num_pixels * sizeof(float_type));
  cudaMalloc((void **)&(res->input_sup), num_pixels * sizeof(float_type));

  cudaMemcpy(res->input_inf, tmp_input_inf, num_pixels * sizeof(float_type),
             cudaMemcpyHostToDevice);
  cudaMemcpy(res->input_sup, tmp_input_sup, num_pixels * sizeof(float_type),
             cudaMemcpyHostToDevice);

  free(tmp_input_inf);
  free(tmp_input_sup);

  res->num_pixels = num_pixels;
}

elina_abstract0_t *fppoly_from_network_input(elina_manager_t *man,
                                             const size_t intdim,
                                             const size_t realdim,
                                             const double *inf_array,
                                             const double *sup_array) {
  fppoly_t *res = (fppoly_t *)malloc(sizeof(fppoly_t));
  fppoly_from_network_input_box(res, intdim, realdim, inf_array, sup_array);

  return abstract0_of_fppoly(man, res);
}

void fppoly_add_new_layer(fppoly_t *const fp, const size_t num_out_neurons,
                          const size_t num_in_neurons, const layertype_t type,
                          const activation_type_t activation) {
  const size_t numlayers = fp->numlayers;
  fp->layers[numlayers] =
      create_layer(num_out_neurons, num_in_neurons, type, activation);
  fp->numlayers++;
}

__device__ void elina_double_interval_add_expr_coeff(
    float_type *const res_inf, float_type *const res_sup, const float_type inf,
    const float_type sup, const float_type inf_expr,
    const float_type sup_expr) {
  *res_inf = inf + inf_expr;
  *res_sup = sup + sup_expr;
  const float_type maxA = fmax(fabs(inf_expr), fabs(sup_expr));
  float_type tmp1, tmp2;
  elina_double_interval_mul(&tmp1, &tmp2, inf, sup, maxA * ulp, maxA * ulp);
  *res_inf += tmp1;
  *res_sup += tmp2;
}

__device__ void elina_double_interval_add_cst_coeff(float_type *const res_inf,
                                                    float_type *const res_sup,
                                                    const float_type inf,
                                                    const float_type sup,
                                                    const float_type inf_expr,
                                                    const float_type sup_expr) {
  elina_double_interval_add_expr_coeff(res_inf, res_sup, inf, sup, inf_expr,
                                       sup_expr);
  *res_inf += min_denormal;
  *res_sup += min_denormal;
}

__device__ void elina_double_interval_mul_expr_coeff(
    float_type *const res_inf, float_type *const res_sup, const float_type inf,
    const float_type sup, const float_type inf_expr,
    const float_type sup_expr) {
  elina_double_interval_mul(res_inf, res_sup, inf, sup, inf_expr, sup_expr);
  const float_type maxA = fmax(fabs(inf_expr), fabs(sup_expr));
  float_type tmp1, tmp2;
  elina_double_interval_mul(&tmp1, &tmp2, inf, sup, maxA * ulp, maxA * ulp);
  *res_inf += tmp1;
  *res_sup += tmp2;
}

__device__ void elina_double_interval_mul_cst_coeff(float_type *const res_inf,
                                                    float_type *const res_sup,
                                                    const float_type inf,
                                                    const float_type sup,
                                                    const float_type inf_expr,
                                                    const float_type sup_expr) {
  elina_double_interval_mul_expr_coeff(res_inf, res_sup, inf, sup, inf_expr,
                                       sup_expr);
  *res_inf += min_denormal;
  *res_sup += min_denormal;
}

__global__ void compute_lb_from_expr(float_type *__restrict__ lb_array,
                                     const float_type *__restrict__ inf_coeff,
                                     const float_type *__restrict__ sup_coeff,
                                     const float_type *__restrict__ inf_cst,
                                     const float_type *__restrict__ input_inf,
                                     const float_type *__restrict__ input_sup,
                                     const size_t num_exprs,
                                     const size_t expr_size) {
  const size_t n = blockIdx.x;

  float_type res_inf = inf_cst[n];

  float_type tmp1, tmp2;

  for (size_t i = 0; i < expr_size; i++) {
    elina_double_interval_mul(&tmp1, &tmp2, inf_coeff[n * expr_size + i],
                              sup_coeff[n * expr_size + i], input_inf[i],
                              input_sup[i]);
    res_inf = res_inf + tmp1;
  }

  lb_array[n] = res_inf;
}

__global__ void compute_ub_from_expr(float_type *__restrict__ ub_array,
                                     const float_type *__restrict__ inf_coeff,
                                     const float_type *__restrict__ sup_coeff,
                                     const float_type *__restrict__ sup_cst,
                                     const float_type *__restrict__ input_inf,
                                     const float_type *__restrict__ input_sup,
                                     const size_t num_exprs,
                                     const size_t expr_size) {
  const size_t n = blockIdx.x;

  float_type res_sup = sup_cst[n];

  float_type tmp1, tmp2;

  for (size_t i = 0; i < expr_size; i++) {
    elina_double_interval_mul(&tmp1, &tmp2, inf_coeff[n * expr_size + i],
                              sup_coeff[n * expr_size + i], input_inf[i],
                              input_sup[i]);
    res_sup = res_sup + tmp2;
  }

  ub_array[n] = res_sup;
}

__global__ void device_layer_create_dense_expr(
    float_type *__restrict__ inf_coeff, float_type *__restrict__ sup_coeff,
    float_type *__restrict__ inf_cst, float_type *__restrict__ sup_cst,
    const double *__restrict__ weights, const double *__restrict__ bias,
    const size_t num_out_neurons, const size_t num_in_neurons) {
  const size_t i = blockIdx.x;

  const double *weight_i = weights + i * num_in_neurons;
  const double bias_i = bias[i];

  inf_cst[i] = -bias_i;
  sup_cst[i] = bias_i;

  for (size_t j = 0; j < num_in_neurons; j++) {
    inf_coeff[i * num_in_neurons + j] = -weight_i[j];
    sup_coeff[i * num_in_neurons + j] = weight_i[j];
  }
}

void layer_create_dense_exprs(float_type *inf_coeff, float_type *sup_coeff,
                              float_type *inf_cst, float_type *sup_cst,
                              const double **weights, const double *bias,
                              const size_t num_out_neurons,
                              const size_t num_in_neurons) {
  double *tmp_weights;
  cudaMalloc((void **)&tmp_weights,
             num_out_neurons * num_in_neurons * sizeof(double));

  double *tmp_bias;
  cudaMalloc((void **)&tmp_bias, num_out_neurons * sizeof(double));

  for (size_t i = 0; i < num_out_neurons; i++) {
    cudaMemcpy(tmp_weights + i * num_in_neurons, weights[i],
               num_in_neurons * sizeof(double), cudaMemcpyHostToDevice);
  }

  cudaMemcpy(tmp_bias, bias, num_out_neurons * sizeof(double),
             cudaMemcpyHostToDevice);

  device_layer_create_dense_expr<<<num_out_neurons, 1>>>(
      inf_coeff, sup_coeff, inf_cst, sup_cst, tmp_weights, tmp_bias,
      num_out_neurons, num_in_neurons);

  cudaFree(tmp_weights);
  cudaFree(tmp_bias);
}

__global__ void copy_expr_array(float_type *__restrict__ target_inf_coeff,
                                float_type *__restrict__ target_sup_coeff,
                                float_type *__restrict__ target_inf_cst,
                                float_type *__restrict__ target_sup_cst,
                                const float_type *__restrict__ source_inf_coeff,
                                const float_type *__restrict__ source_sup_coeff,
                                const float_type *__restrict__ source_inf_cst,
                                const float_type *__restrict__ source_sup_cst,
                                const size_t num_exprs,
                                const size_t expr_size) {
  const size_t i = blockIdx.x;

  for (size_t j = 0; j < expr_size; j++) {
    target_inf_coeff[i * expr_size + j] = source_inf_coeff[i * expr_size + j];
    target_sup_coeff[i * expr_size + j] = source_sup_coeff[i * expr_size + j];
  }

  target_inf_cst[i] = source_inf_cst[i];
  target_sup_cst[i] = source_sup_cst[i];
}

void layer_compute_bounds_from_exprs(
    float_type *inf_coeff, float_type *sup_coeff, float_type *inf_cst,
    float_type *sup_cst, float_type *lb_array, float_type *ub_array,
    float_type *input_inf, float_type *input_sup, const size_t num_out_neurons,
    const size_t num_in_neurons) {
  compute_lb_from_expr<<<num_out_neurons, 1>>>(lb_array, inf_coeff, sup_coeff,
                                               inf_cst, input_inf, input_sup,
                                               num_out_neurons, num_in_neurons);
  compute_ub_from_expr<<<num_out_neurons, 1>>>(ub_array, inf_coeff, sup_coeff,
                                               sup_cst, input_inf, input_sup,
                                               num_out_neurons, num_in_neurons);
}

void ffn_handle_first_layer(elina_manager_t *man, elina_abstract0_t *abs,
                            const double **weights, const double *bias,
                            const size_t size, const size_t num_pixels,
                            const activation_type_t activation) {
  fppoly_t *res = fppoly_of_abstract0(abs);
  fppoly_internal_t *pr =
      fppoly_init_from_manager(man, ELINA_FUNID_ASSIGN_LINEXPR_ARRAY);

  res->layers = (layer_t **)malloc(20 * sizeof(layer_t *));
  fppoly_add_new_layer(res, size, num_pixels, FFN, activation);

  float_type *inf_coeff = res->layers[0]->inf_coeff;
  float_type *sup_coeff = res->layers[0]->sup_coeff;

  float_type *inf_cst = res->layers[0]->inf_cst;
  float_type *sup_cst = res->layers[0]->sup_cst;

  layer_create_dense_exprs(inf_coeff, sup_coeff, inf_cst, sup_cst, weights,
                           bias, size, num_pixels);
  layer_compute_bounds_from_exprs(
      inf_coeff, sup_coeff, inf_cst, sup_cst, res->layers[0]->lb_array,
      res->layers[0]->ub_array, res->input_inf, res->input_sup,
      res->layers[0]->num_out_neurons, res->layers[0]->num_in_neurons);
}

void ffn_handle_first_relu_layer(elina_manager_t *man, elina_abstract0_t *abs,
                                 const double **weights, const double *bias,
                                 const size_t size, const size_t num_pixels) {
  ffn_handle_first_layer(man, abs, weights, bias, size, num_pixels, RELU);
}

void ffn_handle_first_sigmoid_layer(elina_manager_t *man,
                                    elina_abstract0_t *abs,
                                    const double **weights, const double *bias,
                                    const size_t size,
                                    const size_t num_pixels) {
  // ffn_handle_first_layer(man, abs, weights, bias, size, num_pixels, SIGMOID);
}

void ffn_handle_first_tanh_layer(elina_manager_t *man, elina_abstract0_t *abs,
                                 const double **weights, const double *bias,
                                 const size_t size, const size_t num_pixels) {
  // ffn_handle_first_layer(man, abs, weights, bias, size, num_pixels, TANH);
}

__global__ void lexpr_replace_relu_bounds(
    float_type *__restrict__ inf_coeff, float_type *__restrict__ sup_coeff,
    float_type *__restrict__ inf_cst, float_type *__restrict__ sup_cst,
    const float_type *__restrict__ lb_array,
    const float_type *__restrict__ ub_array,
    const size_t num_out_neurons_last_layer,
    const size_t num_out_neurons_current_layer) {
  const size_t n = blockIdx.x;
  const size_t i = blockIdx.y * blockDim.x + threadIdx.x;

  if (i < num_out_neurons_current_layer) {
    const size_t a = n * num_out_neurons_current_layer + i;

    const float_type lb = lb_array[i];
    const float_type ub = ub_array[i];
    const float_type width = ub + lb;
    const float_type lambda_inf = -ub / width;
    const float_type lambda_sup = ub / width;

    const float_type old_inf_coeff = inf_coeff[a];
    const float_type old_sup_coeff = sup_coeff[a];

    if ((old_sup_coeff == 0) && (old_inf_coeff == 0)) {
      inf_coeff[a] = 0.0;
      sup_coeff[a] = 0.0;

      return;
    } else if (ub <= 0) {
      inf_coeff[a] = 0.0;
      sup_coeff[a] = 0.0;

      return;
    } else if (lb < 0) {
      inf_coeff[a] = old_inf_coeff;
      sup_coeff[a] = old_sup_coeff;
    } else if (old_sup_coeff < 0) {
      const float_type mu_inf = lambda_inf * lb;
      const float_type mu_sup = lambda_sup * lb;
      elina_double_interval_mul_expr_coeff(&inf_coeff[a], &sup_coeff[a],
                                           lambda_inf, lambda_sup,
                                           old_inf_coeff, old_sup_coeff);
      float_type tmp1, tmp2;
      elina_double_interval_mul_cst_coeff(&tmp1, &tmp2, mu_inf, mu_sup,
                                          old_inf_coeff, old_sup_coeff);

      atomicAdd(&inf_cst[n], tmp1 + min_denormal);
      atomicAdd(&sup_cst[n], tmp2 + min_denormal);
    } else if (old_inf_coeff < 0) {
      const float_type area1 = lb * ub;
      const float_type area2 = 0.5 * ub * width;
      const float_type area3 = 0.5 * lb * width;

      if ((area1 < area2) && (area1 < area3)) {
        elina_double_interval_mul_expr_coeff(&inf_coeff[a], &sup_coeff[a],
                                             lambda_inf, lambda_sup,
                                             old_inf_coeff, old_sup_coeff);
      } else if ((area2 < area1) && (area2 < area3)) {
        inf_coeff[a] = 0.0;
        sup_coeff[a] = 0.0;
      } else {
        inf_coeff[a] = old_inf_coeff;
        sup_coeff[a] = old_sup_coeff;
      }
    } else {
      inf_coeff[a] = 0.0;
      sup_coeff[a] = 0.0;
      float_type tmp1, tmp2;
      elina_double_interval_mul(&tmp1, &tmp2, old_inf_coeff, old_sup_coeff, 0,
                                ub);

      atomicAdd(&inf_cst[n], tmp1);
      atomicAdd(&sup_cst[n], -tmp1);
    }
  }
}

__global__ void uexpr_replace_relu_bounds(
    float_type *__restrict__ inf_coeff, float_type *__restrict__ sup_coeff,
    float_type *__restrict__ inf_cst, float_type *__restrict__ sup_cst,
    const float_type *__restrict__ lb_array,
    const float_type *__restrict__ ub_array,
    const size_t num_out_neurons_last_layer,
    const size_t num_out_neurons_current_layer) {
  const size_t n = blockIdx.x;
  const size_t i = blockIdx.y * blockDim.x + threadIdx.x;

  if (i < num_out_neurons_current_layer) {
    const size_t a = n * num_out_neurons_current_layer + i;

    const float_type lb = lb_array[i];
    const float_type ub = ub_array[i];
    const float_type width = ub + lb;
    const float_type lambda_inf = -ub / width;
    const float_type lambda_sup = ub / width;

    const float_type old_inf_coeff = inf_coeff[a];
    const float_type old_sup_coeff = sup_coeff[a];

    if ((old_sup_coeff == 0) && (old_inf_coeff == 0)) {
      inf_coeff[a] = 0.0;
      sup_coeff[a] = 0.0;

      return;
    } else if (ub <= 0) {
      inf_coeff[a] = 0.0;
      sup_coeff[a] = 0.0;

      return;
    } else if (lb < 0) {
      inf_coeff[a] = old_inf_coeff;
      sup_coeff[a] = old_sup_coeff;
    } else if (old_inf_coeff < 0) {
      const float_type mu_inf = lambda_inf * lb;
      const float_type mu_sup = lambda_sup * lb;
      elina_double_interval_mul_expr_coeff(&inf_coeff[a], &sup_coeff[a],
                                           lambda_inf, lambda_sup,
                                           old_inf_coeff, old_sup_coeff);
      float_type tmp1, tmp2;
      elina_double_interval_mul_cst_coeff(&tmp1, &tmp2, mu_inf, mu_sup,
                                          old_inf_coeff, old_sup_coeff);

      atomicAdd(&inf_cst[n], tmp1 + min_denormal);
      atomicAdd(&sup_cst[n], tmp2 + min_denormal);
    } else if (old_sup_coeff < 0) {
      const float_type area1 = lb * ub;
      const float_type area2 = 0.5 * ub * width;
      const float_type area3 = 0.5 * lb * width;

      if ((area1 < area2) && (area1 < area3)) {
        elina_double_interval_mul_expr_coeff(&inf_coeff[a], &sup_coeff[a],
                                             lambda_inf, lambda_sup,
                                             old_inf_coeff, old_sup_coeff);
      } else if ((area2 < area1) && (area2 < area3)) {
        inf_coeff[a] = 0.0;
        sup_coeff[a] = 0.0;
      } else {
        inf_coeff[a] = old_inf_coeff;
        sup_coeff[a] = old_sup_coeff;
      }
    } else {
      inf_coeff[a] = 0.0;
      sup_coeff[a] = 0.0;
      float_type tmp1, tmp2;
      elina_double_interval_mul(&tmp1, &tmp2, old_inf_coeff, old_sup_coeff, 0,
                                ub);

      atomicAdd(&inf_cst[n], -tmp2);
      atomicAdd(&sup_cst[n], tmp2);
    }
  }
}

// TODO: Try to load values from aux-array only once and use them multiple
// times!
__global__ void
coeffs_from_previous_layer(const float_type *__restrict__ expr_inf_coeff,
                           const float_type *__restrict__ expr_sup_coeff,
                           float_type *__restrict__ res_inf_coeff,
                           float_type *__restrict__ res_sup_coeff,
                           const float_type *__restrict__ aux_inf_coeff,
                           const float_type *__restrict__ aux_sup_coeff,
                           const size_t num_out_neurons_last_layer,
                           const size_t num_out_neurons_current_layer,
                           const size_t num_in_neurons_current_layer) {
  const size_t n = blockIdx.x;
  const size_t j = blockIdx.y * blockDim.x + threadIdx.x;

  if (j < num_in_neurons_current_layer) {
    size_t i = 0;

    size_t a = n * num_out_neurons_current_layer + i;
    const size_t b = n * num_in_neurons_current_layer + j;
    size_t c = i * num_in_neurons_current_layer + j;

    float_type inf_coeff;
    float_type sup_coeff;

    elina_double_interval_mul_expr_coeff(&inf_coeff, &sup_coeff,
                                         expr_inf_coeff[a], expr_sup_coeff[a],
                                         aux_inf_coeff[c], aux_sup_coeff[c]);

    float_type tmp1, tmp2;
    float_type maxRes, maxMul;

    for (i = 1; i < num_out_neurons_current_layer; i++) {
      a++;
      c += num_in_neurons_current_layer;

      const float_type prev_inf_coeff = expr_inf_coeff[a];
      const float_type prev_sup_coeff = expr_sup_coeff[a];

      if ((prev_inf_coeff != 0) || (prev_sup_coeff != 0)) {
        elina_double_interval_mul_expr_coeff(&tmp1, &tmp2, prev_inf_coeff,
                                             prev_sup_coeff, aux_inf_coeff[c],
                                             aux_sup_coeff[c]);

        maxRes = fmax(fabs(inf_coeff), fabs(sup_coeff));
        maxMul = fmax(fabs(tmp1), fabs(tmp2));

        inf_coeff = inf_coeff + tmp1 + (maxRes + maxMul) * ulp;
        sup_coeff = sup_coeff + tmp2 + (maxRes + maxMul) * ulp;
      }
    }

    res_inf_coeff[b] = inf_coeff;
    res_sup_coeff[b] = sup_coeff;
  }
}

__global__ void
csts_from_previous_layer(const float_type *__restrict__ expr_inf_coeff,
                         const float_type *__restrict__ expr_sup_coeff,
                         const float_type *__restrict__ expr_inf_cst,
                         const float_type *__restrict__ expr_sup_cst,
                         float_type *__restrict__ res_inf_cst,
                         float_type *__restrict__ res_sup_cst,
                         const float_type *__restrict__ aux_inf_cst,
                         const float_type *__restrict__ aux_sup_cst,
                         const size_t num_out_neurons_last_layer,
                         const size_t num_out_neurons_current_layer) {
  const size_t n = blockIdx.x;

  size_t i = 0;

  size_t a = n * num_out_neurons_current_layer + i;

  float_type inf_cst;
  float_type sup_cst;

  elina_double_interval_mul_cst_coeff(&inf_cst, &sup_cst, expr_inf_coeff[a],
                                      expr_sup_coeff[a], aux_inf_cst[i],
                                      aux_sup_cst[i]);

  float_type tmp1, tmp2;
  float_type maxRes, maxMul;

  for (i = 1; i < num_out_neurons_current_layer; i++) {
    a++;

    const float_type prev_inf_coeff = expr_inf_coeff[a];
    const float_type prev_sup_coeff = expr_sup_coeff[a];

    if ((prev_inf_coeff != 0) || (prev_sup_coeff != 0)) {
      elina_double_interval_mul_cst_coeff(&tmp1, &tmp2, prev_inf_coeff,
                                          prev_sup_coeff, aux_inf_cst[i],
                                          aux_sup_cst[i]);

      maxRes = fmax(fabs(inf_cst), fabs(sup_cst));
      maxMul = fmax(fabs(tmp1), fabs(tmp2));

      inf_cst += tmp1 + (maxRes + maxMul) * ulp + min_denormal;
      sup_cst += tmp2 + (maxRes + maxMul) * ulp + min_denormal;
    }
  }

  res_inf_cst[n] = inf_cst + expr_inf_cst[n];
  res_sup_cst[n] = sup_cst + expr_sup_cst[n];
}

void update_state_using_previous_layers(elina_manager_t *man, fppoly_t *fp,
                                        const size_t layerno) {
  auto start = std::chrono::system_clock::now();

  fppoly_internal_t *pr =
      fppoly_init_from_manager(man, ELINA_FUNID_ASSIGN_LINEXPR_ARRAY);

  const size_t num_out_neurons_last_layer =
      fp->layers[layerno]->num_out_neurons;
  const size_t num_in_neurons_last_layer = fp->layers[layerno]->num_in_neurons;

  const size_t num_in_neurons_first_layer = fp->layers[0]->num_in_neurons;

  std::cout << "num_out_neurons_last " << num_out_neurons_last_layer
            << std::endl;

  float_type *inf_coeff = fp->layers[layerno]->inf_coeff;
  float_type *sup_coeff = fp->layers[layerno]->sup_coeff;

  float_type *inf_cst = fp->layers[layerno]->inf_cst;
  float_type *sup_cst = fp->layers[layerno]->sup_cst;

  float_type *lb_array = fp->layers[layerno]->lb_array;
  float_type *ub_array = fp->layers[layerno]->ub_array;

  float_type *linf_coeff;
  float_type *lsup_coeff;
  float_type *linf_cst;
  float_type *lsup_cst;

  cudaMalloc((void **)&linf_coeff, num_out_neurons_last_layer *
                                       num_in_neurons_last_layer *
                                       sizeof(float_type));
  cudaMalloc((void **)&lsup_coeff, num_out_neurons_last_layer *
                                       num_in_neurons_last_layer *
                                       sizeof(float_type));
  cudaMalloc((void **)&linf_cst,
             num_out_neurons_last_layer * sizeof(float_type));
  cudaMalloc((void **)&lsup_cst,
             num_out_neurons_last_layer * sizeof(float_type));

  float_type *uinf_coeff;
  float_type *usup_coeff;
  float_type *uinf_cst;
  float_type *usup_cst;

  cudaMalloc((void **)&uinf_coeff, num_out_neurons_last_layer *
                                       num_in_neurons_last_layer *
                                       sizeof(float_type));
  cudaMalloc((void **)&usup_coeff, num_out_neurons_last_layer *
                                       num_in_neurons_last_layer *
                                       sizeof(float_type));
  cudaMalloc((void **)&uinf_cst,
             num_out_neurons_last_layer * sizeof(float_type));
  cudaMalloc((void **)&usup_cst,
             num_out_neurons_last_layer * sizeof(float_type));

  copy_expr_array<<<num_out_neurons_last_layer, 1>>>(
      linf_coeff, lsup_coeff, linf_cst, lsup_cst, inf_coeff, sup_coeff, inf_cst,
      sup_cst, num_out_neurons_last_layer, num_in_neurons_last_layer);
  copy_expr_array<<<num_out_neurons_last_layer, 1>>>(
      uinf_coeff, usup_coeff, uinf_cst, usup_cst, inf_coeff, sup_coeff, inf_cst,
      sup_cst, num_out_neurons_last_layer, num_in_neurons_last_layer);

  float_type *linf_coeff_tmp;
  float_type *lsup_coeff_tmp;
  float_type *linf_cst_tmp;
  float_type *lsup_cst_tmp;

  float_type *uinf_coeff_tmp;
  float_type *usup_coeff_tmp;
  float_type *uinf_cst_tmp;
  float_type *usup_cst_tmp;

  cudaMalloc((void **)&linf_cst_tmp,
             num_out_neurons_last_layer * sizeof(float_type));
  cudaMalloc((void **)&lsup_cst_tmp,
             num_out_neurons_last_layer * sizeof(float_type));
  cudaMalloc((void **)&uinf_cst_tmp,
             num_out_neurons_last_layer * sizeof(float_type));
  cudaMalloc((void **)&usup_cst_tmp,
             num_out_neurons_last_layer * sizeof(float_type));

  for (int k = layerno - 1; k >= 0; k--) {
    const size_t num_out_neurons_current_layer = fp->layers[k]->num_out_neurons;
    const size_t num_in_neurons_current_layer = fp->layers[k]->num_in_neurons;
    std::cout << "num_out_neurons_current " << num_out_neurons_current_layer
              << " num_in_neurons_current " << num_in_neurons_current_layer
              << std::endl;

    const dim3 num_blocks_relu(num_out_neurons_last_layer,
                               num_out_neurons_current_layer / num_threads + 1,
                               1);
    const dim3 num_blocks_linear(num_out_neurons_last_layer,
                                 num_in_neurons_current_layer / num_threads + 1,
                                 1);

    std::cout << "num_threads" << num_threads << " num_blocks_relu "
              << num_blocks_relu.y << " num_blocks_linear "
              << num_blocks_linear.y << std::endl;

    float_type *aux_inf_coeff = fp->layers[k]->inf_coeff;
    float_type *aux_sup_coeff = fp->layers[k]->sup_coeff;

    float_type *aux_inf_cst = fp->layers[k]->inf_cst;
    float_type *aux_sup_cst = fp->layers[k]->sup_cst;

    float_type *aux_lb_array = fp->layers[k]->lb_array;
    float_type *aux_ub_array = fp->layers[k]->ub_array;

    if (fp->layers[k]->activation == RELU) {
      lexpr_replace_relu_bounds<<<num_blocks_relu, num_threads>>>(
          linf_coeff, lsup_coeff, linf_cst, lsup_cst, aux_lb_array,
          aux_ub_array, num_out_neurons_last_layer,
          num_out_neurons_current_layer);
      uexpr_replace_relu_bounds<<<num_blocks_relu, num_threads>>>(
          uinf_coeff, usup_coeff, uinf_cst, usup_cst, aux_lb_array,
          aux_ub_array, num_out_neurons_last_layer,
          num_out_neurons_current_layer);
    }

    cudaMalloc((void **)&linf_coeff_tmp, num_out_neurons_last_layer *
                                             num_in_neurons_current_layer *
                                             sizeof(float_type));
    cudaMalloc((void **)&lsup_coeff_tmp, num_out_neurons_last_layer *
                                             num_in_neurons_current_layer *
                                             sizeof(float_type));
    cudaMalloc((void **)&uinf_coeff_tmp, num_out_neurons_last_layer *
                                             num_in_neurons_current_layer *
                                             sizeof(float_type));
    cudaMalloc((void **)&usup_coeff_tmp, num_out_neurons_last_layer *
                                             num_in_neurons_current_layer *
                                             sizeof(float_type));

    coeffs_from_previous_layer<<<num_blocks_linear, num_threads>>>(
        linf_coeff, lsup_coeff, linf_coeff_tmp, lsup_coeff_tmp, aux_inf_coeff,
        aux_sup_coeff, num_out_neurons_last_layer,
        num_out_neurons_current_layer, num_in_neurons_current_layer);
    coeffs_from_previous_layer<<<num_blocks_linear, num_threads>>>(
        uinf_coeff, usup_coeff, uinf_coeff_tmp, usup_coeff_tmp, aux_inf_coeff,
        aux_sup_coeff, num_out_neurons_last_layer,
        num_out_neurons_current_layer, num_in_neurons_current_layer);

    csts_from_previous_layer<<<num_out_neurons_last_layer, 1>>>(
        linf_coeff, lsup_coeff, linf_cst, lsup_cst, linf_cst_tmp, lsup_cst_tmp,
        aux_inf_cst, aux_sup_cst, num_out_neurons_last_layer,
        num_out_neurons_current_layer);
    csts_from_previous_layer<<<num_out_neurons_last_layer, 1>>>(
        uinf_coeff, usup_coeff, uinf_cst, usup_cst, uinf_cst_tmp, usup_cst_tmp,
        aux_inf_cst, aux_sup_cst, num_out_neurons_last_layer,
        num_out_neurons_current_layer);

    std::swap(linf_coeff, linf_coeff_tmp);
    std::swap(lsup_coeff, lsup_coeff_tmp);
    std::swap(linf_cst, linf_cst_tmp);
    std::swap(lsup_cst, lsup_cst_tmp);

    std::swap(uinf_coeff, uinf_coeff_tmp);
    std::swap(usup_coeff, usup_coeff_tmp);
    std::swap(uinf_cst, uinf_cst_tmp);
    std::swap(usup_cst, usup_cst_tmp);

    cudaFree(linf_coeff_tmp);
    cudaFree(lsup_coeff_tmp);
    cudaFree(uinf_coeff_tmp);
    cudaFree(usup_coeff_tmp);
  }

  compute_lb_from_expr<<<num_out_neurons_last_layer, 1>>>(
      lb_array, linf_coeff, lsup_coeff, linf_cst, fp->input_inf, fp->input_sup,
      num_out_neurons_last_layer, num_in_neurons_first_layer);
  compute_ub_from_expr<<<num_out_neurons_last_layer, 1>>>(
      ub_array, uinf_coeff, usup_coeff, usup_cst, fp->input_inf, fp->input_sup,
      num_out_neurons_last_layer, num_in_neurons_first_layer);

  cudaFree(linf_coeff);
  cudaFree(lsup_coeff);
  cudaFree(linf_cst);
  cudaFree(lsup_cst);

  cudaFree(uinf_coeff);
  cudaFree(usup_coeff);
  cudaFree(uinf_cst);
  cudaFree(usup_cst);

  cudaFree(linf_coeff_tmp);
  cudaFree(lsup_coeff_tmp);
  cudaFree(linf_cst_tmp);
  cudaFree(lsup_cst_tmp);

  cudaFree(uinf_coeff_tmp);
  cudaFree(usup_coeff_tmp);
  cudaFree(uinf_cst_tmp);
  cudaFree(usup_cst_tmp);

  cudaDeviceSynchronize();

  auto end = std::chrono::system_clock::now();

  std::chrono::duration<double> elapsed_seconds = end - start;
  std::cout << "elapsed time: " << elapsed_seconds.count() << "s" << std::endl
            << std::endl;
}

void ffn_handle_intermediate_layer(elina_manager_t *man,
                                   elina_abstract0_t *element,
                                   const double **weights, const double *bias,
                                   const size_t num_out_neurons,
                                   const size_t num_in_neurons,
                                   const activation_type_t activation) {
  fppoly_t *fp = fppoly_of_abstract0(element);
  fppoly_add_new_layer(fp, num_out_neurons, num_in_neurons, FFN, activation);

  float_type *inf_coeff = fp->layers[fp->numlayers - 1]->inf_coeff;
  float_type *sup_coeff = fp->layers[fp->numlayers - 1]->sup_coeff;

  float_type *inf_cst = fp->layers[fp->numlayers - 1]->inf_cst;
  float_type *sup_cst = fp->layers[fp->numlayers - 1]->sup_cst;

  layer_create_dense_exprs(inf_coeff, sup_coeff, inf_cst, sup_cst, weights,
                           bias, num_out_neurons, num_in_neurons);

  update_state_using_previous_layers(man, fp, fp->numlayers - 1);
}

void ffn_handle_intermediate_relu_layer(elina_manager_t *man,
                                        elina_abstract0_t *element,
                                        const double **weights,
                                        const double *bias,
                                        const size_t num_out_neurons,
                                        const size_t num_in_neurons) {
  ffn_handle_intermediate_layer(man, element, weights, bias, num_out_neurons,
                                num_in_neurons, RELU);
}

void ffn_handle_intermediate_sigmoid_layer(elina_manager_t *man,
                                           elina_abstract0_t *element,
                                           const double **weights,
                                           const double *bias,
                                           const size_t num_out_neurons,
                                           const size_t num_in_neurons) {
  // ffn_handle_intermediate_layer(man, element, weights, bias, num_out_neurons,
  // num_in_neurons, SIGMOID);
}

void ffn_handle_intermediate_tanh_layer(elina_manager_t *man,
                                        elina_abstract0_t *element,
                                        const double **weights,
                                        const double *bias,
                                        const size_t num_out_neurons,
                                        const size_t num_in_neurons) {
  // ffn_handle_intermediate_layer(man, element, weights, bias, num_out_neurons,
  // num_in_neurons, TANH);
}

__global__ void print_bounds(const float_type *__restrict__ bounds_array,
                             const size_t num_out_neurons) {
  for (size_t i = 0; i < num_out_neurons; i++) {
    printf("out inf number %i is: %g\n", i, bounds_array[i]);
  }
}

void ffn_handle_last_layer(elina_manager_t *man, elina_abstract0_t *element,
                           const double **weights, const double *bias,
                           const size_t num_out_neurons,
                           const size_t num_in_neurons,
                           const bool has_activation,
                           const activation_type_t activation) {
  fppoly_t *fp = fppoly_of_abstract0(element);
  fppoly_internal_t *pr =
      fppoly_init_from_manager(man, ELINA_FUNID_ASSIGN_LINEXPR_ARRAY);

  if (has_activation) {
    fppoly_add_new_layer(fp, num_out_neurons, num_in_neurons, FFN, activation);
  } else {
    fppoly_add_new_layer(fp, num_out_neurons, num_in_neurons, FFN, NONE);
  }

  float_type *inf_coeff = fp->layers[fp->numlayers - 1]->inf_coeff;
  float_type *sup_coeff = fp->layers[fp->numlayers - 1]->sup_coeff;

  float_type *inf_cst = fp->layers[fp->numlayers - 1]->inf_cst;
  float_type *sup_cst = fp->layers[fp->numlayers - 1]->sup_cst;

  layer_create_dense_exprs(inf_coeff, sup_coeff, inf_cst, sup_cst, weights,
                           bias, num_out_neurons, num_in_neurons);

  update_state_using_previous_layers(man, fp, fp->numlayers - 1);

  float_type *lb_array = fp->layers[fp->numlayers - 1]->lb_array;
  float_type *ub_array = fp->layers[fp->numlayers - 1]->ub_array;

  print_bounds<<<1, 1>>>(lb_array, num_out_neurons);
  print_bounds<<<1, 1>>>(ub_array, num_out_neurons);
}

void ffn_handle_last_relu_layer(elina_manager_t *man,
                                elina_abstract0_t *element,
                                const double **weights, const double *bias,
                                const size_t num_out_neurons,
                                const size_t num_in_neurons,
                                const bool has_relu) {
  ffn_handle_last_layer(man, element, weights, bias, num_out_neurons,
                        num_in_neurons, has_relu, RELU);
}

void ffn_handle_last_sigmoid_layer(elina_manager_t *man,
                                   elina_abstract0_t *element,
                                   const double **weights, const double *bias,
                                   const size_t num_out_neurons,
                                   const size_t num_in_neurons,
                                   const bool has_sigmoid) {
  // ffn_handle_last_layer(man, element, weights, bias, num_out_neurons,
  // num_in_neurons, has_sigmoid, SIGMOID);
}

void ffn_handle_last_tanh_layer(elina_manager_t *man,
                                elina_abstract0_t *element,
                                const double **weights, const double *bias,
                                const size_t num_out_neurons,
                                const size_t num_in_neurons,
                                const bool has_tanh) {
  // ffn_handle_last_layer(man, element, weights, bias, num_out_neurons,
  // num_in_neurons, has_tanh, TANH);
}

__global__ void create_sub_expr(float_type *__restrict__ inf_coeff,
                                float_type *__restrict__ sup_coeff,
                                float_type *__restrict__ inf_cst,
                                float_type *__restrict__ sup_cst,
                                const size_t index, const elina_dim_t y,
                                const elina_dim_t x) {
  inf_cst[index] = 0;
  sup_cst[index] = 0;

  for (size_t i = 0; i < 10; i++) {
    inf_coeff[index * 10 + i] = 0.;
    sup_coeff[index * 10 + i] = 0.;
  }

  inf_coeff[index * 10 + y] = -1.;
  sup_coeff[index * 10 + y] = 1.;

  inf_coeff[index * 10 + x] = 1.;
  sup_coeff[index * 10 + x] = -1.;
}

void get_lb_using_previous_layers(elina_manager_t *man,
                                  const fppoly_t *const fp) {
  const size_t numlayers = fp->numlayers;
  fppoly_internal_t *pr =
      fppoly_init_from_manager(man, ELINA_FUNID_ASSIGN_LINEXPR_ARRAY);

  const size_t num_out_neurons_last_layer = 90;

  const size_t num_in_neurons_first_layer = fp->layers[0]->num_in_neurons;

  float_type *lb_dev;
  cudaMalloc((void **)&lb_dev, num_out_neurons_last_layer * sizeof(float_type));

  float_type *linf_coeff;
  float_type *lsup_coeff;
  float_type *linf_cst;
  float_type *lsup_cst;

  cudaMalloc((void **)&linf_coeff,
             num_out_neurons_last_layer * 10 * sizeof(float_type *));
  cudaMalloc((void **)&lsup_coeff,
             num_out_neurons_last_layer * 10 * sizeof(float_type *));
  cudaMalloc((void **)&linf_cst,
             num_out_neurons_last_layer * 10 * sizeof(float_type));
  cudaMalloc((void **)&lsup_cst,
             num_out_neurons_last_layer * 10 * sizeof(float_type));

  size_t index = 0;

  for (elina_dim_t y = 0; y < 10; y++) {
    for (elina_dim_t x = 0; x < 10; x++) {
      if (y != x) {
        create_sub_expr<<<1, 1>>>(linf_coeff, lsup_coeff, linf_cst, lsup_cst,
                                  index, y, x);
        index++;
      }
    }
  }

  float_type *linf_coeff_tmp;
  float_type *lsup_coeff_tmp;
  float_type *linf_cst_tmp;
  float_type *lsup_cst_tmp;

  cudaMalloc((void **)&linf_coeff_tmp,
             num_out_neurons_last_layer * sizeof(float_type *));
  cudaMalloc((void **)&lsup_coeff_tmp,
             num_out_neurons_last_layer * sizeof(float_type *));
  cudaMalloc((void **)&linf_cst_tmp,
             num_out_neurons_last_layer * sizeof(float_type));
  cudaMalloc((void **)&lsup_cst_tmp,
             num_out_neurons_last_layer * sizeof(float_type));

  for (int k = numlayers - 1; k >= 0; k--) {
    const size_t num_out_neurons_current_layer = fp->layers[k]->num_out_neurons;
    const size_t num_in_neurons_current_layer = fp->layers[k]->num_in_neurons;

    const dim3 num_blocks_relu(num_out_neurons_last_layer,
                               num_out_neurons_current_layer / num_threads + 1,
                               1);
    const dim3 num_blocks_linear(num_out_neurons_last_layer,
                                 num_in_neurons_current_layer / num_threads + 1,
                                 1);

    float_type *aux_inf_coeff = fp->layers[k]->inf_coeff;
    float_type *aux_sup_coeff = fp->layers[k]->sup_coeff;

    float_type *aux_inf_cst = fp->layers[k]->inf_cst;
    float_type *aux_sup_cst = fp->layers[k]->sup_cst;

    float_type *aux_lb_array = fp->layers[k]->lb_array;
    float_type *aux_ub_array = fp->layers[k]->ub_array;

    if (fp->layers[k]->activation == RELU) {
      lexpr_replace_relu_bounds<<<num_blocks_relu, num_threads>>>(
          linf_coeff, lsup_coeff, linf_cst, lsup_cst, aux_lb_array,
          aux_ub_array, num_out_neurons_last_layer,
          num_out_neurons_current_layer);
    }

    cudaMalloc((void **)&linf_coeff_tmp, num_out_neurons_last_layer *
                                             num_in_neurons_current_layer *
                                             sizeof(float_type));
    cudaMalloc((void **)&lsup_coeff_tmp, num_out_neurons_last_layer *
                                             num_in_neurons_current_layer *
                                             sizeof(float_type));

    coeffs_from_previous_layer<<<num_blocks_linear, num_threads>>>(
        linf_coeff, lsup_coeff, linf_coeff_tmp, lsup_coeff_tmp, aux_inf_coeff,
        aux_sup_coeff, num_out_neurons_last_layer,
        num_out_neurons_current_layer, num_in_neurons_current_layer);

    csts_from_previous_layer<<<num_out_neurons_last_layer, 1>>>(
        linf_coeff, lsup_coeff, linf_cst, lsup_cst, linf_cst_tmp, lsup_cst_tmp,
        aux_inf_cst, aux_sup_cst, num_out_neurons_last_layer,
        num_out_neurons_current_layer);

    std::swap(linf_coeff, linf_coeff_tmp);
    std::swap(lsup_coeff, lsup_coeff_tmp);
    std::swap(linf_cst, linf_cst_tmp);
    std::swap(lsup_cst, lsup_cst_tmp);

    cudaFree(linf_coeff_tmp);
    cudaFree(lsup_coeff_tmp);
  }

  compute_lb_from_expr<<<num_out_neurons_last_layer, 1>>>(
      lb_dev, linf_coeff, lsup_coeff, linf_cst, fp->input_inf, fp->input_sup,
      num_out_neurons_last_layer, num_in_neurons_first_layer);

  cudaFree(linf_coeff);
  cudaFree(lsup_coeff);
  cudaFree(linf_cst);
  cudaFree(lsup_cst);

  cudaFree(linf_coeff_tmp);
  cudaFree(lsup_coeff_tmp);
  cudaFree(linf_cst_tmp);
  cudaFree(lsup_cst_tmp);

  float_type lb[num_out_neurons_last_layer];
  cudaMemcpy(&lb, lb_dev, num_out_neurons_last_layer * sizeof(float_type),
             cudaMemcpyDeviceToHost);

  cudaFree(lb_dev);

  for (size_t i = 0; i < num_out_neurons_last_layer; i++) {
    if (lb[i] < 0) {
      results[i] = true;
    } else {
      results[i] = false;
    }
  }
}

bool is_greater(elina_manager_t *man, elina_abstract0_t *element,
                const elina_dim_t y, const elina_dim_t x) {
  const fppoly_t *fp = fppoly_of_abstract0(element);
  fppoly_internal_t *pr =
      fppoly_init_from_manager(man, ELINA_FUNID_ASSIGN_LINEXPR_ARRAY);

  if (!results_calculated) {
    get_lb_using_previous_layers(man, fp);
    results_calculated = true;

    return results[0];
  } else {
    bool result = results[output_counter];
    output_counter++;

    return result;
  }
}

void device_layer_create_sparse_exprs(
    float_type *inf_coeff, float_type *sup_coeff, float_type *inf_cst,
    float_type *sup_cst, const double *filter_weights,
    const double *filter_bias, const size_t *input_size,
    const size_t *output_size, const size_t *filter_size, const size_t *strides,
    const long int pad_top, const long int pad_left, const size_t num_pixels) {
  const size_t num_out_neurons =
      output_size[0] * output_size[1] * output_size[2];

  double *dense_coeff =
      (double *)calloc(num_out_neurons * num_pixels, sizeof(double));
  double *bias = (double *)calloc(num_out_neurons, sizeof(double));

  for (size_t out_x = 0; out_x < output_size[0]; out_x++) {
    for (size_t out_y = 0; out_y < output_size[1]; out_y++) {
      for (size_t out_z = 0; out_z < output_size[2]; out_z++) {
        const size_t mat_x = out_x * output_size[1] * output_size[2] +
                             out_y * output_size[2] + out_z;

        for (size_t x_shift = 0; x_shift < filter_size[0]; x_shift++) {
          for (size_t y_shift = 0; y_shift < filter_size[1]; y_shift++) {
            for (size_t inp_z = 0; inp_z < input_size[2]; inp_z++) {
              const long int x_val = out_x * strides[0] + x_shift - pad_top;
              const long int y_val = out_y * strides[1] + y_shift - pad_left;

              if ((y_val < 0) || (y_val >= (long int)input_size[1])) {
                continue;
              }

              if ((x_val < 0) || (x_val >= (long int)input_size[0])) {
                continue;
              }

              const size_t mat_y = x_val * input_size[1] * input_size[2] +
                                   y_val * input_size[2] + inp_z;

              if (mat_y >= num_pixels) {
                continue;
              }

              const size_t filter_index =
                  x_shift * filter_size[1] * input_size[2] * output_size[2] +
                  y_shift * input_size[2] * output_size[2] +
                  inp_z * output_size[2] + out_z;
              dense_coeff[mat_x * num_pixels + mat_y] =
                  filter_weights[filter_index];
            }
          }
        }

        bias[mat_x] = filter_bias[out_z];
      }
    }
  }

  double *dense_coeff_dev;
  double *bias_dev;

  cudaMalloc((void **)&dense_coeff_dev,
             num_out_neurons * num_pixels * sizeof(double));
  cudaMalloc((void **)&bias_dev, num_out_neurons * sizeof(double));

  cudaMemcpy(dense_coeff_dev, dense_coeff,
             num_out_neurons * num_pixels * sizeof(double),
             cudaMemcpyHostToDevice);
  cudaMemcpy(bias_dev, bias, num_out_neurons * sizeof(double),
             cudaMemcpyHostToDevice);

  device_layer_create_dense_expr<<<num_out_neurons, 1>>>(
      inf_coeff, sup_coeff, inf_cst, sup_cst, dense_coeff_dev, bias_dev,
      num_out_neurons, num_pixels);

  cudaFree(dense_coeff_dev);
  cudaFree(bias_dev);

  free(dense_coeff);
  free(bias);
}

void layer_create_sparse_exprs(fppoly_t *const fp, const double *filter_weights,
                               const double *filter_bias,
                               const size_t *input_size,
                               const size_t *filter_size,
                               const size_t num_filters, const size_t *strides,
                               const bool is_valid_padding,
                               const bool has_bias) {
  const size_t num_pixels = input_size[0] * input_size[1] * input_size[2];

  size_t output_size[3];

  if (is_valid_padding) {
    output_size[0] =
        ceil((double)(input_size[0] - filter_size[0] + 1) / (double)strides[0]);
    output_size[1] =
        ceil((double)(input_size[1] - filter_size[1] + 1) / (double)strides[1]);
  } else {
    output_size[0] = ceil((double)input_size[0] / (double)strides[0]);
    output_size[1] = ceil((double)input_size[1] / (double)strides[1]);
  }

  output_size[2] = num_filters;

  const size_t num_out_neurons =
      output_size[0] * output_size[1] * output_size[2];
  fppoly_add_new_layer(fp, num_out_neurons, num_pixels, CONV, RELU);

  float_type *inf_coeff = fp->layers[fp->numlayers - 1]->inf_coeff;
  float_type *sup_coeff = fp->layers[fp->numlayers - 1]->sup_coeff;

  float_type *inf_cst = fp->layers[fp->numlayers - 1]->inf_cst;
  float_type *sup_cst = fp->layers[fp->numlayers - 1]->sup_cst;

  long int pad_along_height = 0;
  long int pad_along_width = 0;
  long int pad_top = 0;
  long int pad_left = 0;

  if (!is_valid_padding) {
    if (input_size[0] % strides[0] == 0) {
      const long int tmp = filter_size[0] - strides[0];
      pad_along_height = max(tmp, long(0));
    } else {
      const long int tmp = filter_size[0] - (input_size[0] % strides[0]);
      pad_along_height = max(tmp, long(0));
    }

    if (input_size[1] % strides[1] == 0) {
      const long int tmp = filter_size[1] - strides[1];
      pad_along_width = max(tmp, long(0));
    } else {
      const long int tmp = filter_size[1] - (input_size[1] % strides[1]);
      pad_along_width = max(tmp, long(0));
    }

    pad_top = pad_along_height / 2;
    pad_left = pad_along_width / 2;
  }

  const size_t size =
      filter_size[0] * filter_size[1] * input_size[2] * output_size[2];

  double *filter_weights_tmp = (double *)malloc(size * sizeof(double));
  double *filter_bias_tmp = (double *)calloc(output_size[2], sizeof(double));

  size_t *input_size_tmp = (size_t *)malloc(3 * sizeof(size_t));
  size_t *output_size_tmp = (size_t *)malloc(3 * sizeof(size_t));
  size_t *filter_size_tmp = (size_t *)malloc(2 * sizeof(size_t));
  size_t *strides_tmp = (size_t *)malloc(2 * sizeof(size_t));

  cudaMemcpy(filter_weights_tmp, filter_weights, size * sizeof(double),
             cudaMemcpyHostToHost);

  if (has_bias) {
    cudaMemcpy(filter_bias_tmp, filter_bias, output_size[2] * sizeof(double),
               cudaMemcpyHostToHost);
  }

  cudaMemcpy(input_size_tmp, input_size, 3 * sizeof(size_t),
             cudaMemcpyHostToHost);
  cudaMemcpy(output_size_tmp, output_size, 3 * sizeof(size_t),
             cudaMemcpyHostToHost);
  cudaMemcpy(filter_size_tmp, filter_size, 2 * sizeof(size_t),
             cudaMemcpyHostToHost);
  cudaMemcpy(strides_tmp, strides, 2 * sizeof(size_t), cudaMemcpyHostToHost);

  device_layer_create_sparse_exprs(
      inf_coeff, sup_coeff, inf_cst, sup_cst, filter_weights_tmp,
      filter_bias_tmp, input_size_tmp, output_size_tmp, filter_size_tmp,
      strides_tmp, pad_top, pad_left, num_pixels);

  free(filter_weights_tmp);
  free(filter_bias_tmp);

  free(input_size_tmp);
  free(output_size_tmp);
  free(filter_size_tmp);
  free(strides_tmp);
}

void conv_handle_first_layer(elina_manager_t *man, elina_abstract0_t *element,
                             const double *filter_weights,
                             const double *filter_bias,
                             const size_t *input_size,
                             const size_t *filter_size,
                             const size_t num_filters, const size_t *strides,
                             const bool is_valid_padding, const bool has_bias) {
  fppoly_t *const fp = fppoly_of_abstract0(element);
  fp->layers = (layer_t **)malloc(20 * sizeof(layer_t *));

  layer_create_sparse_exprs(fp, filter_weights, filter_bias, input_size,
                            filter_size, num_filters, strides, is_valid_padding,
                            has_bias);

  float_type *inf_coeff = fp->layers[0]->inf_coeff;
  float_type *sup_coeff = fp->layers[0]->sup_coeff;

  float_type *inf_cst = fp->layers[0]->inf_cst;
  float_type *sup_cst = fp->layers[0]->sup_cst;

  layer_compute_bounds_from_exprs(
      inf_coeff, sup_coeff, inf_cst, sup_cst, fp->layers[0]->lb_array,
      fp->layers[0]->ub_array, fp->input_inf, fp->input_sup,
      fp->layers[0]->num_out_neurons, fp->layers[0]->num_in_neurons);
}

void conv_handle_intermediate_relu_layer(
    elina_manager_t *man, elina_abstract0_t *element,
    const double *filter_weights, const double *filter_bias,
    const size_t *input_size, const size_t *filter_size,
    const size_t num_filters, const size_t *strides,
    const bool is_valid_padding, const bool has_bias) {
  fppoly_t *const fp = fppoly_of_abstract0(element);

  layer_create_sparse_exprs(fp, filter_weights, filter_bias, input_size,
                            filter_size, num_filters, strides, is_valid_padding,
                            has_bias);

  update_state_using_previous_layers(man, fp, fp->numlayers - 1);
}

void free_layer(layer_t *layer) {
  cudaFree(layer->inf_coeff);
  cudaFree(layer->sup_coeff);
  cudaFree(layer->inf_cst);
  cudaFree(layer->sup_cst);

  layer->inf_coeff = nullptr;
  layer->sup_coeff = nullptr;
  layer->inf_cst = nullptr;
  layer->sup_cst = nullptr;

  cudaFree(layer->lb_array);
  cudaFree(layer->ub_array);

  layer->lb_array = nullptr;
  layer->ub_array = nullptr;

  free(layer);
  layer = nullptr;
}

void fppoly_free(elina_manager_t *man, fppoly_t *fp) {
  for (size_t i = 0; i < fp->numlayers; i++) {
    free_layer(fp->layers[i]);
  }

  free(fp->layers);
  fp->layers = nullptr;

  cudaFree(fp->input_inf);
  fp->input_inf = nullptr;
  cudaFree(fp->input_sup);
  fp->input_sup = nullptr;

  free(fp);
  fp = nullptr;
}

void layer_print(const layer_t *layer) {
  // neurons_print<<<1, 1>>>(layer->neurons, layer->num_out_neurons);
}

void fppoly_fprint(FILE *const stream, elina_manager_t *man,
                   const fppoly_t *const fp, const char **name_of_dim) {
  for (size_t i = 0; i < fp->numlayers; i++) {
    printf("layer: %zu\n", i);
    layer_print(fp->layers[i]);
  }
}
